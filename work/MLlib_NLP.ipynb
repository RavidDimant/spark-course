{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6edb9ee-c031-431c-a8c3-5c0d519beb2c",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "qdj3dYJixxAc"
   },
   "source": [
    "# NLP Via Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d4503a-5311-4f9a-8941-a26a472b6b2f",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "i3tSRnbTxxAe"
   },
   "source": [
    "Spark can assist us in NLP applications!\n",
    "\n",
    "For more information go to: https://spark.apache.org/docs/latest/ml-features.html\n",
    "\n",
    "In this notebook we will show 3 applications of natural language processing (NLP) using Spark's MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0fd2c3-7c09-4354-8671-23ee729b9537",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "eQ0SAknYxxAe"
   },
   "source": [
    "## TF -IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94685b7a-5095-487f-89c9-bb33d4e0c402",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "S3yfjrqZxxAf"
   },
   "source": [
    "*Reminder:* TF-IDF (Term Frequency-Inverse Document Frequency), is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents, by considering both the frequency of the word in the document and the frequency of the word in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Term Frequency (TF):\n",
    "\n",
    "*   Term Frequency measures how often a term (word) appears in a document. It is calculated using the formula:\n",
    "*   TF(t,d)=\n",
    "(Total number of terms in document d) /\n",
    "(Number of times term t appears in document d)\n",
    "*   TF gives higher weight to terms that appear more frequently within a document. It helps in identifying the importance of a word in a specific document.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "\n",
    "\n",
    "*   Inverse Document Frequency measures the importance of a term across a collection of documents. It is calculated using the formula:\n",
    "*   IDF(t,D)=log(\n",
    "(Number of documents containing term t) /\n",
    "(Total number of documents in corpus ∣D∣)\n",
    " )\n",
    "*   IDF gives higher weight to terms that are rare across the entire corpus but are present in a few documents. It helps in identifying terms that are unique or specific to certain documents.\n",
    "\n",
    "TF-IDF Calculation:\n",
    "\n",
    "\n",
    "*   TF-IDF combines the TF and IDF values to determine the weight of a term in a document relative to the entire corpus. It is calculated as:\n",
    "*    TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)\n",
    "*  TF-IDF increases with the number of times a term appears in a document (TF) but is offset by the rarity of the term across the corpus (IDF). This helps in identifying terms that are both frequent in a document and unique to that document.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1PHn4wTfPG1H",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Data\n",
    "Note that we must not share copyrighted data, without prior written permission from the owner.\n",
    "Our data consists of some of the best books written by F. Scott Fitzgerald, including the renowned work \"The Great Gatsby.\" These books were obtained from The Project Gutenberg eBook, an extensive digital library initiative that provides free access to a wide range of public domain literary works.\n",
    "\n",
    "You can find Project Gutenberg at https://www.gutenberg.org/"
   ],
   "metadata": {
    "id": "Ihq2I7D55Kcj",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TF-IDF Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the list of document names and their respective paths\n",
    "documents = {\n",
    "    \"TheGreatGatsby\": \"../data/TheGreatGatsby.txt\",\n",
    "    \"ThisSideofParadise\": \"../data/ThisSideofParadise.txt\",\n",
    "    \"TheBeautifulandDamned\": \"../data/TheBeautifulandDamned.txt\",\n",
    "    \"TalesofTheJazzAge\": \"../data/TalesofTheJazzAge.txt\"\".txt\",\n",
    "    \"FlappersandPhilosophers\": \"../data/FlappersandPhilosophers.txt\",\n",
    "    \"AlltheSadYoungMen\": \"../data/AlltheSadYoungMen.txt\",\n",
    "}\n",
    "\n",
    "# Preprocess and tokenize each document\n",
    "unique_words = set()  # Set to store unique words\n",
    "for doc_name, doc_path in documents.items():\n",
    "    with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Preprocess the data\n",
    "    text = text.lower()  # Lowercase all words\n",
    "    words = re.findall(r'\\w+', text)  # Extract words using regular expression\n",
    "\n",
    "    # Add unique words to the set\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Convert set to list to create a DataFrame\n",
    "text_data = spark.createDataFrame([(word,) for word in list(unique_words)], [\"text\"])\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(text_data)\n",
    "\n",
    "# Apply HashingTF to convert words to term frequency vectors\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "tf_data = hashing_tf.transform(words_data)\n",
    "\n",
    "# Compute IDF to get the TF-IDF vectors\n",
    "idf_model = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_data = idf_model.fit(tf_data).transform(tf_data)\n",
    "\n",
    "# Extract the top N words with the highest TF-IDF scores\n",
    "num_top_words = 5\n",
    "top_words = idf_data.select(\"words\", \"features\") \\\n",
    "    .rdd.map(lambda row: row.asDict()) \\\n",
    "    .flatMap(lambda x: [(word, score) for word, score in zip(x[\"words\"], x[\"features\"])]) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(num_top_words)\n",
    "\n",
    "# Print the top N unique words with their TF-IDF scores\n",
    "print(f\"Top {num_top_words} unique words based on TF-IDF scores across the corpus:\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word}: {score:.3f}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJDnrljAEX1Y",
    "outputId": "21080e98-2188-4b0a-f8f4-b61f8868db41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 5 unique words based on TF-IDF scores across the corpus:\n",
      "diffusing: 6.967\n",
      "decent: 6.967\n",
      "skeins: 6.967\n",
      "hope: 6.967\n",
      "unimpeachable: 6.967\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8d2f84-ad7a-4550-b091-be22d950e4a1",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "iWCY5B4QxxAi"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1181b4d5-9572-4216-b945-739b8f88d760",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "U5nReqPuxxAi"
   },
   "source": [
    "*Reminder:* Word2Vec is a technique for learning distributed representations of words in a continuous vector space from large text corpora. It associates words with vectors in such a way that semantically similar words are mapped to nearby points in the vector space, enabling algorithms to capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Word2VecExample\").getOrCreate()\n",
    "\n",
    "# Prepare input data: Each row is a bag of words from a sentence or document\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# FlatMap the array of words to separate rows\n",
    "words_df = documentDF.rdd.flatMap(lambda x: x[0])\n",
    "# Collect the words into a list\n",
    "word_list = words_df.collect()\n",
    "\n",
    "# Learn a mapping from words to vectors\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "word_vectors = model.getVectors()\n",
    "\n",
    "# Create a dictionary mapping words to vectors\n",
    "word_to_vector = {}\n",
    "for row in word_vectors.collect():\n",
    "    word = row[\"word\"]\n",
    "    vector = DenseVector(row[\"vector\"])\n",
    "    word_to_vector[word] = vector\n",
    "\n",
    "# Print each word and its Word2Vec representation\n",
    "for word, vector in word_to_vector.items():\n",
    "    print(\"Word: '{}'\\nVector: {}\".format(word, vector))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4658ahhALok",
    "outputId": "ae2145df-1dac-4e11-f583-39ae75019776",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word: 'heard'\n",
      "Vector: [-0.1428680270910263,0.052687663584947586,0.10879096388816833]\n",
      "Word: 'are'\n",
      "Vector: [0.12437053769826889,0.14396001398563385,-0.15560980141162872]\n",
      "Word: 'neat'\n",
      "Vector: [-0.06944344192743301,0.16405947506427765,-0.13333088159561157]\n",
      "Word: 'classes'\n",
      "Vector: [0.10187613219022751,-0.14191721379756927,0.08895421773195267]\n",
      "Word: 'I'\n",
      "Vector: [0.11645925045013428,-0.04665014520287514,0.005499431863427162]\n",
      "Word: 'regression'\n",
      "Vector: [0.0016052094288170338,-0.0849643424153328,0.01142392959445715]\n",
      "Word: 'Logistic'\n",
      "Vector: [0.12236311286687851,-0.04140022397041321,0.08744674175977707]\n",
      "Word: 'Spark'\n",
      "Vector: [-0.018199866637587547,-0.07334957271814346,0.15221886336803436]\n",
      "Word: 'could'\n",
      "Vector: [0.12124157696962357,0.0344112291932106,0.007167116738855839]\n",
      "Word: 'use'\n",
      "Vector: [-0.15078683197498322,0.027394283562898636,-0.049794115126132965]\n",
      "Word: 'Hi'\n",
      "Vector: [-0.0015435452805832028,-0.13844861090183258,-0.01282623689621687]\n",
      "Word: 'models'\n",
      "Vector: [-0.02154526300728321,-0.0933031439781189,0.08284977823495865]\n",
      "Word: 'case'\n",
      "Vector: [0.14803075790405273,-0.15597927570343018,-0.0721140056848526]\n",
      "Word: 'about'\n",
      "Vector: [0.06857839226722717,0.028209982439875603,-0.11975220590829849]\n",
      "Word: 'Java'\n",
      "Vector: [-0.07946175336837769,0.1041347086429596,0.012727963738143444]\n",
      "Word: 'wish'\n",
      "Vector: [-0.05768797546625137,-0.11903265118598938,0.008259364403784275]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = float(vec1.dot(vec2))\n",
    "    norm_vec1 = float(vec1.norm(2))\n",
    "    norm_vec2 = float(vec2.norm(2))\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Calculate cosine similarity between specific words\n",
    "word_pairs = [(\"Hi\", \"heard\"), (\"Spark\", \"Java\"), (\"regression\", \"neat\")]\n",
    "\n",
    "# Get the list of words\n",
    "words = [row[\"word\"] for row in word_vectors.collect()]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    word1_vec = word_vectors.filter(word_vectors.word == pair[0]).select(\"vector\").collect()[0][0]\n",
    "    word2_vec = word_vectors.filter(word_vectors.word == pair[1]).select(\"vector\").collect()[0][0]\n",
    "    similarity = cosine_similarity(DenseVector(word1_vec), DenseVector(word2_vec))\n",
    "    print(\"Cosine similarity between '{}' and '{}': {:.4f}\".format(pair[0], pair[1], similarity))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDEBT7LWBkUC",
    "outputId": "82c39fef-d189-406e-c171-a38e04adcda0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cosine similarity between 'Hi' and 'heard': -0.3255\n",
      "Cosine similarity between 'Spark' and 'Java': -0.1902\n",
      "Cosine similarity between 'regression' and 'neat': -0.8163\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example was not very informative. Let's try again with a larger amount of data."
   ],
   "metadata": {
    "id": "wlSI5U4NhkPC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alice in Wonderland Example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SfWJvA5VxxAk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Word2VecExample\").getOrCreate()\n",
    "\n",
    "# Load the text file \"Alice_book.txt\" and preprocess sentences\n",
    "TEXT_PATH= \"../data/Alice_book.txt\"\n",
    "with open(TEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "sentences = [sen.strip().lower() for sen in sentences]  # Lowercase all words\n",
    "sentences = [sen.split() for sen in sentences if sen]  # Split sentences into words\n",
    "sentences = [[re.sub(r'\\W+', '', w) for w in sen] for sen in sentences]  # Remove non-word characters\n",
    "\n",
    "# Prepare input data: Each row is a bag of words from a sentence or document\n",
    "documentDF = spark.createDataFrame([(sen,) for sen in sentences], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to vectors\n",
    "word2Vec = Word2Vec(vectorSize=1000, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "word_vectors = model.getVectors()\n",
    "\n",
    "# Create a dictionary mapping words to vectors\n",
    "word_to_vector = {}\n",
    "for row in word_vectors.collect():\n",
    "    word = row[\"word\"]\n",
    "    vector = DenseVector(row[\"vector\"])\n",
    "    word_to_vector[word] = vector\n",
    "\n"
   ],
   "metadata": {
    "id": "dDvUBTBVJB_N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cosine distance between words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2zebIy_bxxAk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- \"tea\" and \"time\" should have high similarity as they are closely related in the context of \"tea-time\" in \"Alice in Wonderland\".\n",
    "- \"queen\" and \"hearts\" should have high similarity as they are closely related in the context of the Queen of Hearts character.\n",
    "- \"hatter\" and \"watch\" should have low similarity as they could represent distinct concepts or contexts. In the book, the rabbit had a watch.\n",
    "- \"alice\" and \"dream\" might have low similarity as they are less likely to co-occur in the text.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "9c8roAMTxxAk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cosine similarity between 'queen' and 'hearts': 0.9460\n",
      "Cosine similarity between 'tea' and 'time': 0.5390\n",
      "Cosine similarity between 'alice' and 'dream': -0.0349\n",
      "Cosine similarity between 'hatter' and 'watch': 0.0255\n"
     ]
    }
   ],
   "source": [
    "# Define the cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = float(vec1.dot(vec2))\n",
    "    norm_vec1 = float(vec1.norm(2))\n",
    "    norm_vec2 = float(vec2.norm(2))\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "# Calculate cosine similarity between specific words\n",
    "word_pairs = [(\"queen\", \"hearts\"), (\"tea\", \"time\"), (\"alice\", \"dream\"), (\"hatter\", \"watch\")]\n",
    "\n",
    "# Get the list of words\n",
    "words = [row[\"word\"] for row in word_vectors.collect()]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    if pair[0] in word_to_vector and pair[1] in word_to_vector:\n",
    "        word1_vec = word_to_vector[pair[0]]\n",
    "        word2_vec = word_to_vector[pair[1]]\n",
    "        similarity = cosine_similarity(DenseVector(word1_vec), DenseVector(word2_vec))\n",
    "        print(\"Cosine similarity between '{}' and '{}': {:.4f}\".format(pair[0], pair[1], similarity))\n",
    "    else:\n",
    "        print(\"One or both words '{}' and '{}' not found in the word vectors.\".format(pair[0], pair[1]))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBxanfYFxxAk",
    "outputId": "8c8d823e-a4be-4470-ec1c-6f8f2707ffa0"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe567e9-67fc-46d9-998e-82d916403fc1",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "3a3xX48fxxAk"
   },
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c50e157-e824-4ad9-a391-54d77535fe05",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "0QKZSzZwxxAl"
   },
   "source": [
    "*Reminder:* N-grams are contiguous sequences of n items (words, characters, or tokens) extracted from a text document, where n represents the number of items in the sequence. They are commonly used in natural language processing for tasks such as language modeling, text generation, and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f00cfc8-b5c2-4963-b9db-5155d1d2b4e3",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9ivtYAqxxAl",
    "outputId": "1819a179-7ad1-4d49-855c-291836e75243"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    from pyspark.ml.feature import NGram\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"NGramExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create a DataFrame with word sequences\n",
    "    wordDataFrame = spark.createDataFrame([\n",
    "        (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "        (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "        (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "    ], [\"id\", \"words\"])\n",
    "\n",
    "    # Generate NGrams from the word sequences\n",
    "    ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "\n",
    "    # Show the generated NGrams\n",
    "    ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encoding\n",
    "\n",
    "We can encode the vectors we got using n-grams for various natural language processing (NLP) tasks such as:\n",
    "- Text Classification\n",
    "- Sentiment Analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Topic Modeling\n",
    "- Machine Translation\n",
    "And more!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "d-F6-uwmxxAl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+------------------------------------------------------------------+----------------------------------------------+\n",
      "|id |ngrams                                                            |features                                      |\n",
      "+---+------------------------------------------------------------------+----------------------------------------------+\n",
      "|0  |[Hi I, I heard, heard about, about Spark]                         |(14,[3,5,10,12],[1.0,1.0,1.0,1.0])            |\n",
      "|1  |[I wish, wish Java, Java could, could use, use case, case classes]|(14,[2,4,6,7,11,13],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|2  |[Logistic regression, regression models, models are, are neat]    |(14,[0,1,8,9],[1.0,1.0,1.0,1.0])              |\n",
      "+---+------------------------------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\")\n",
    "\n",
    "# Fit the CountVectorizer to the NGrams DataFrame\n",
    "cv_model = cv.fit(ngramDataFrame)\n",
    "\n",
    "# Transform the NGrams DataFrame to get the encoded features\n",
    "encoded_df = cv_model.transform(ngramDataFrame)\n",
    "\n",
    "# Show the encoded features\n",
    "encoded_df.select(\"id\", \"ngrams\", \"features\").show(truncate=False)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmtISY2PxxAl",
    "outputId": "fef96ed5-8cd5-4edb-8645-e4f5c7cd307f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's break down the representation (14,[2,4,11,13],[1.0,1.0,1.0,1.0]):\n",
    "\n",
    "14: Total vocabulary size, meaning there are 14 distinct NGrams in the dataset.\n",
    "[2, 4, 11, 13]: Indices of the NGrams that have non-zero counts in the encoded vector. These indices correspond to the positions of NGrams in the vocabulary.\n",
    "[1.0, 1.0, 1.0, 1.0]: Counts of each NGram in the encoded vector. Each count indicates how many times the corresponding NGram appears in the text data.\n",
    "So, in this example, the vector [Hi I, I heard, heard about, about Spark] is represented as a sparse vector with a vocabulary size of 14, and it contains non-zero counts for the NGrams at indices 2, 4, 11, and 13."
   ],
   "metadata": {
    "id": "Sxl9z-mlRgXB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now use the encoded vectors for various purposes such as training machine learning models, performing similarity calculations, or any other task that requires numerical representations of text data."
   ],
   "metadata": {
    "id": "lg0oRnYQSBXj",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Do not forget to release the resources held by Spark\n",
    "spark.stop()"
   ],
   "metadata": {
    "id": "MjGmJyGkS9DE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 31,
   "outputs": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ravid 2024-03-04 15:07:53",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}